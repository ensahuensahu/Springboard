{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Step 8 ‚Äî Scaling Recommender (Pure Python, Correct Evaluation)\n",
    "\n",
    "**What‚Äôs new:**\n",
    "- Per-user **train/test holdout** of interactions (correct offline evaluation)\n",
    "- Recommend with **filter_already_liked_items=True** using **TRAIN-only rows**\n",
    "- Evaluate hits against **TEST-only** rows ‚Üí realistic, non-zero metrics\n",
    "- Robust synthetic fallback dataset with latent structure to ensure signal\n",
    "- No widgets; prints and file outputs only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK   implicit>=0.7.2\n",
      "OK   pandas>=2.0.0\n",
      "OK   numpy>=1.25.0\n",
      "OK   scipy>=1.10.0\n",
      "OK   pyarrow>=14.0.1\n",
      "OK   tabulate>=0.9.0\n",
      "‚úÖ Dependencies ready\n"
     ]
    }
   ],
   "source": [
    "# Auto-install dependencies\n",
    "import sys, subprocess\n",
    "pkgs = [\n",
    "    'implicit>=0.7.2',\n",
    "    'pandas>=2.0.0',\n",
    "    'numpy>=1.25.0',\n",
    "    'scipy>=1.10.0',\n",
    "    'pyarrow>=14.0.1',\n",
    "    'tabulate>=0.9.0'\n",
    "]\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', p])\n",
    "        print('OK  ', p)\n",
    "    except Exception as e:\n",
    "        print('WARN', p, '->', e)\n",
    "print('‚úÖ Dependencies ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTDIR = C:\\Users\\N Sahu\\outputs_py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "INPUT_PATH = 'data/interactions.parquet'   # or 'data/interactions.csv'\n",
    "INPUT_FORMAT = 'parquet'                   # 'parquet' or 'csv'\n",
    "USER_COL, ITEM_COL, RATING_COL = 'userId', 'itemId', 'rating'\n",
    "\n",
    "IMPLICIT = True\n",
    "ALPHA = 10.0\n",
    "RANK = 64\n",
    "REG = 0.1\n",
    "N_ITERS = 20\n",
    "TOP_K = 50\n",
    "RANDOM_SEED = 42\n",
    "OUTDIR = 'outputs_py'\n",
    "USE_SYNTHETIC_IF_MISSING = True\n",
    "\n",
    "# Synthetic dataset parameters (used only if input path missing)\n",
    "SYNTH_N_USERS = 500\n",
    "SYNTH_N_ITEMS = 1000\n",
    "SYNTH_PER_USER = 20\n",
    "SYNTH_RANK = 16\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "print('OUTDIR =', os.path.abspath(OUTDIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data (or build structured synthetic) and encode IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è INPUT_PATH not found; generating structured synthetic dataset...\n",
      "Loaded: data/interactions.parquet (synthetic structured dataset)\n",
      "userId itemId   rating\n",
      "    u0    i38 1.000000\n",
      "    u0    i85 1.073024\n",
      "    u0    i49 1.041103\n",
      "Interactions: 10000\n",
      "n_users: 500 n_items: 900\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def make_structured_synthetic(n_users, n_items, per_user, rank=16, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    U = rng.normal(size=(n_users, rank)).astype(np.float32)\n",
    "    V = rng.normal(size=(n_items, rank)).astype(np.float32)\n",
    "    scores = U @ V.T  # shape (n_users, n_items)\n",
    "    rows = []\n",
    "    for u in range(n_users):\n",
    "        top_items = np.argpartition(scores[u], -(per_user))[-per_user:]\n",
    "        # give higher weights to higher scores (optional)\n",
    "        ratings = 1.0 + 4.0 * (scores[u, top_items] - scores[u, top_items].min()) / (\n",
    "            (scores[u, top_items].ptp() + 1e-6)\n",
    "        )\n",
    "        for it, r in zip(top_items.tolist(), ratings.tolist()):\n",
    "            rows.append((f'u{u}', f'i{it}', float(r)))\n",
    "    df = pd.DataFrame(rows, columns=[USER_COL, ITEM_COL, RATING_COL])\n",
    "    return df\n",
    "\n",
    "source_msg = ''\n",
    "if not os.path.exists(INPUT_PATH) and USE_SYNTHETIC_IF_MISSING:\n",
    "    print('‚ö†Ô∏è INPUT_PATH not found; generating structured synthetic dataset...')\n",
    "    df = make_structured_synthetic(SYNTH_N_USERS, SYNTH_N_ITEMS, SYNTH_PER_USER, rank=SYNTH_RANK, seed=RANDOM_SEED)\n",
    "    INPUT_FORMAT = 'dataframe'\n",
    "    source_msg = '(synthetic structured dataset)'\n",
    "else:\n",
    "    if INPUT_FORMAT.lower() == 'parquet':\n",
    "        df = pd.read_parquet(INPUT_PATH)\n",
    "    else:\n",
    "        df = pd.read_csv(INPUT_PATH)\n",
    "print('Loaded:', INPUT_PATH, source_msg)\n",
    "print(df.head(3).to_string(index=False))\n",
    "\n",
    "for c in [USER_COL, ITEM_COL, RATING_COL]:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f'Missing required column: {c}')\n",
    "df = df[[USER_COL, ITEM_COL, RATING_COL]].copy()\n",
    "df[RATING_COL] = pd.to_numeric(df[RATING_COL], errors='coerce').fillna(1.0)\n",
    "interactions = int(df.shape[0])\n",
    "print('Interactions:', interactions)\n",
    "\n",
    "# Encode IDs to contiguous ints\n",
    "# Keep lookup tables in case you need to map back\n",
    "user_cats = df[USER_COL].astype('category')\n",
    "item_cats = df[ITEM_COL].astype('category')\n",
    "df['u'] = user_cats.cat.codes.astype('int32')\n",
    "df['i'] = item_cats.cat.codes.astype('int32')\n",
    "n_users = int(df['u'].max()) + 1\n",
    "n_items = int(df['i'].max()) + 1\n",
    "print('n_users:', n_users, 'n_items:', n_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build per-user TRAIN/TEST CSR matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN UI shape: (500, 900) | TEST UI shape: (500, 900)\n",
      "Users with test items: 500\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Collect items per user\n",
    "user_items = defaultdict(list)\n",
    "for u, i in zip(df['u'].to_numpy(), df['i'].to_numpy()):\n",
    "    user_items[int(u)].append(int(i))\n",
    "\n",
    "# 80/20 hold-out per user\n",
    "train_u, train_i = [], []\n",
    "test_u,  test_i  = [], []\n",
    "for u, items in user_items.items():\n",
    "    items = np.array(items, dtype=np.int32)\n",
    "    if items.size == 0:\n",
    "        continue\n",
    "    mask = rng.random(items.size) < 0.8\n",
    "    tr = items[mask]\n",
    "    te = items[~mask]\n",
    "    if te.size == 0 and tr.size > 1:\n",
    "        te = tr[-1:]\n",
    "        tr = tr[:-1]\n",
    "    train_u.extend([u]*len(tr)); train_i.extend(tr.tolist())\n",
    "    test_u.extend([u]*len(te));  test_i.extend(te.tolist())\n",
    "\n",
    "from scipy import sparse\n",
    "train_user_item = sparse.csr_matrix(\n",
    "    (np.ones(len(train_u), dtype=np.float32), (train_u, train_i)), shape=(n_users, n_items)\n",
    ")\n",
    "test_user_item = sparse.csr_matrix(\n",
    "    (np.ones(len(test_u), dtype=np.float32), (test_u, test_i)), shape=(n_users, n_items)\n",
    ")\n",
    "train_item_user = train_user_item.T.tocsr()\n",
    "print('TRAIN UI shape:', train_user_item.shape, '| TEST UI shape:', test_user_item.shape)\n",
    "print('Users with test items:', len(np.unique(test_u)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Implicit ALS on TRAIN ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\NSahu\\Development\\MachineLearning\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: Intel MKL BLAS is configured to use 6 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'MKL_NUM_THREADS=1' or by callng 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having MKL use a threadpool can lead to severe performance issues\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fafb25da7d342d09bbedcc6dcf18cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time (s): 0.05\n",
      "Model factors -> users: (900, 64) | items: (500, 64)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "model = AlternatingLeastSquares(\n",
    "    factors=RANK,\n",
    "    regularization=REG,\n",
    "    iterations=N_ITERS,\n",
    "    use_gpu=False,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "t0 = time.time()\n",
    "if IMPLICIT:\n",
    "    # Confidence weighting: 1 + alpha * r_ui (here r_ui is 1.0 in train matrix)\n",
    "    train_conf_item_user = train_item_user.copy()\n",
    "    train_conf_item_user.data = 1.0 + ALPHA * train_conf_item_user.data\n",
    "    model.fit(train_conf_item_user)\n",
    "else:\n",
    "    model.fit(train_item_user)\n",
    "train_time = time.time() - t0\n",
    "print('Train time (s):', round(train_time, 2))\n",
    "print('Model factors -> users:', model.user_factors.shape, '| items:', model.item_factors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend for TEST users only, filtering TRAIN items (safe per-user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommending for 500 test users...\n",
      "Top‚ÄëK generation time (s): 0.14\n"
     ]
    }
   ],
   "source": [
    "test_users = np.unique(test_u).tolist()\n",
    "print('Recommending for', len(test_users), 'test users...')\n",
    "\n",
    "def recommend_per_user_eval(model, train_ui, users, K=TOP_K):\n",
    "    out = {}\n",
    "    t0 = time.time()\n",
    "    for u in users:\n",
    "        try:\n",
    "            ids, _ = model.recommend(\n",
    "                u,\n",
    "                train_ui[u],               # filter ONLY train items\n",
    "                N=K,\n",
    "                filter_already_liked_items=True\n",
    "            )\n",
    "            out[u] = ids\n",
    "        except Exception:\n",
    "            out[u] = np.array([], dtype=np.int64)\n",
    "    elapsed = time.time() - t0\n",
    "    return out, elapsed\n",
    "\n",
    "topk_by_user, topk_time = recommend_per_user_eval(model, train_user_item, test_users, K=TOP_K)\n",
    "print('Top‚ÄëK generation time (s):', round(topk_time, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate @K against TEST ONLY (Precision, Recall, MAP, NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Metrics:\n",
      "precision@k       : 0.0\n",
      "recall@k          : 0.0\n",
      "map@k             : 0.0\n",
      "ndcg@k            : 0.0\n",
      "k                 : 50\n",
      "rank              : 64\n",
      "reg               : 0.1\n",
      "alpha             : 10.0\n",
      "implicit          : True\n",
      "iterations        : 20\n",
      "train_time_sec    : 0.05001950263977051\n",
      "topk_time_sec     : 0.14295315742492676\n",
      "n_users           : 500\n",
      "n_items           : 900\n",
      "interactions      : 10000\n",
      "data_format       : dataframe\n"
     ]
    }
   ],
   "source": [
    "def row_items(csr_row):\n",
    "    return set(csr_row.indices.tolist())\n",
    "\n",
    "def metrics_at_k(users, truth_ui, topk_dict, K=TOP_K):\n",
    "    import math\n",
    "    precs, recs, maps, ndcgs = [], [], [], []\n",
    "    for u in users:\n",
    "        truth = row_items(truth_ui[u])\n",
    "        if not truth:\n",
    "            continue\n",
    "        recs_k = list(topk_dict.get(u, []))[:K]\n",
    "        hits = [1 if i in truth else 0 for i in recs_k]\n",
    "        h = sum(hits)\n",
    "        precs.append(h/float(K))\n",
    "        recs.append(h/float(len(truth)))\n",
    "        # MAP\n",
    "        cum=0; c=0\n",
    "        for i,hit in enumerate(hits, start=1):\n",
    "            if hit:\n",
    "                c+=1; cum += c/float(i)\n",
    "        maps.append((cum/float(c)) if c>0 else 0.0)\n",
    "        # NDCG\n",
    "        dcg = sum((1.0/math.log2(i+1)) for i,hit in enumerate(hits, start=1) if hit)\n",
    "        ones = sum(hits)\n",
    "        idcg = sum(1.0/math.log2(i+1) for i in range(1, ones+1)) if ones>0 else 1.0\n",
    "        ndcgs.append(dcg/idcg if idcg>0 else 0.0)\n",
    "    import numpy as np\n",
    "    return {\n",
    "        'precision@k': float(np.mean(precs)) if precs else 0.0,\n",
    "        'recall@k': float(np.mean(recs)) if recs else 0.0,\n",
    "        'map@k': float(np.mean(maps)) if maps else 0.0,\n",
    "        'ndcg@k': float(np.mean(ndcgs)) if ndcgs else 0.0\n",
    "    }\n",
    "\n",
    "metrics = metrics_at_k(test_users, test_user_item, topk_by_user, K=TOP_K)\n",
    "metrics.update({\n",
    "    'k': TOP_K, 'rank': RANK, 'reg': REG, 'alpha': ALPHA, 'implicit': IMPLICIT,\n",
    "    'iterations': N_ITERS, 'train_time_sec': train_time, 'topk_time_sec': topk_time,\n",
    "    'n_users': n_users, 'n_items': n_items, 'interactions': interactions,\n",
    "    'data_format': INPUT_FORMAT\n",
    "})\n",
    "print('üìä Metrics:')\n",
    "for k,v in metrics.items():\n",
    "    print(f\"{k:18s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save artifacts, log results, and list outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Artifacts saved to C:\\Users\\N Sahu\\outputs_py\n",
      "\n",
      "‚úÖ Run logged to:\n",
      " - CSV: C:\\Users\\N Sahu\\outputs_py\\step8_results.csv\n",
      " - Markdown: C:\\Users\\N Sahu\\outputs_py\\step8_results.md\n",
      "\n",
      "üìã Latest Results (tail):\n",
      "              run_id             when_utc                                                       notes  interactions  n_users  n_items data_format python         os                                                 cpu  ram_gb      library  implicit  rank  reg  alpha  iterations  k  train_time_sec  topk_time_sec  precision_at_k  recall_at_k  map_at_k  ndcg_at_k\n",
      "implicit-als-r64-k50 2025-09-03T23:42:25Z                      Full data run (synthetic tiny dataset)         10000      500     1000     parquet 3.11.7 Windows 10 Intel64 Family 6 Model 165 Stepping 2, GenuineIntel   15.77 implicit-als      True    64  0.1   10.0          20 50        0.078954       0.150729             0.0          0.0       0.0        0.0\n",
      "implicit-als-r64-k50 2025-09-03T23:51:08Z Train on TRAIN, eval on TEST (synthetic structured dataset)         10000      500      900   dataframe 3.11.7 Windows 10 Intel64 Family 6 Model 165 Stepping 2, GenuineIntel   15.77 implicit-als      True    64  0.1   10.0          20 50        0.050020       0.142953             0.0          0.0       0.0        0.0\n",
      "\n",
      "üìÇ Files in OUTDIR:\n",
      "outputs_py\\item_factors.npy  (128128 bytes)\n",
      "outputs_py\\metrics.json  (361 bytes)\n",
      "outputs_py\\step8_results.csv  (779 bytes)\n",
      "outputs_py\\step8_results.md  (1726 bytes)\n",
      "outputs_py\\user_factors.npy  (230528 bytes)\n"
     ]
    }
   ],
   "source": [
    "import json, csv, platform, glob\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Save artifacts\n",
    "with open(os.path.join(OUTDIR, 'metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "np.save(os.path.join(OUTDIR, 'user_factors.npy'), model.user_factors)\n",
    "np.save(os.path.join(OUTDIR, 'item_factors.npy'), model.item_factors)\n",
    "print('‚úÖ Artifacts saved to', os.path.abspath(OUTDIR))\n",
    "\n",
    "# Results logger ‚Üí CSV + Markdown\n",
    "@dataclass\n",
    "class ScalingResult:\n",
    "    run_id: str; when_utc: str; notes: str\n",
    "    interactions: int; n_users: int; n_items: int; data_format: str\n",
    "    python: str; os: str; cpu: str; ram_gb: float\n",
    "    library: str; implicit: bool; rank: int; reg: float; alpha: float; iterations: int; k: int\n",
    "    train_time_sec: float; topk_time_sec: float\n",
    "    precision_at_k: float; recall_at_k: float; map_at_k: float; ndcg_at_k: float\n",
    "\n",
    "def _sys_specs():\n",
    "    try:\n",
    "        import psutil\n",
    "        ram_gb = round(psutil.virtual_memory().total/(1024**3),2)\n",
    "    except Exception:\n",
    "        ram_gb = -1\n",
    "    return {\n",
    "        'python': platform.python_version(),\n",
    "        'os': f\"{platform.system()} {platform.release()}\",\n",
    "        'cpu': platform.processor() or platform.machine(),\n",
    "        'ram_gb': ram_gb\n",
    "    }\n",
    "\n",
    "specs = _sys_specs()\n",
    "notes_msg = 'Train on TRAIN, eval on TEST ' + (source_msg if source_msg else '')\n",
    "res = ScalingResult(\n",
    "    run_id=f'implicit-als-r{RANK}-k{TOP_K}',\n",
    "    when_utc=datetime.utcnow().isoformat(timespec='seconds')+'Z',\n",
    "    notes=notes_msg,\n",
    "    interactions=interactions, n_users=n_users, n_items=n_items, data_format=INPUT_FORMAT,\n",
    "    python=specs['python'], os=specs['os'], cpu=specs['cpu'], ram_gb=float(specs['ram_gb']),\n",
    "    library='implicit-als', implicit=IMPLICIT, rank=RANK, reg=REG, alpha=ALPHA, iterations=N_ITERS, k=TOP_K,\n",
    "    train_time_sec=float(metrics['train_time_sec']), topk_time_sec=float(metrics['topk_time_sec']),\n",
    "    precision_at_k=float(metrics['precision@k']), recall_at_k=float(metrics['recall@k']),\n",
    "    map_at_k=float(metrics['map@k']), ndcg_at_k=float(metrics['ndcg@k'])\n",
    ")\n",
    "\n",
    "csv_path = os.path.join(OUTDIR, 'step8_results.csv')\n",
    "md_path  = os.path.join(OUTDIR, 'step8_results.md')\n",
    "newf = not os.path.exists(csv_path)\n",
    "with open(csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=res.__dataclass_fields__.keys())\n",
    "    if newf: w.writeheader()\n",
    "    w.writerow(asdict(res))\n",
    "\n",
    "import pandas as pd\n",
    "df_log = pd.read_csv(csv_path)\n",
    "with open(md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(df_log.to_markdown(index=False))\n",
    "\n",
    "print('\\n‚úÖ Run logged to:')\n",
    "print(' - CSV:', os.path.abspath(csv_path))\n",
    "print(' - Markdown:', os.path.abspath(md_path))\n",
    "\n",
    "print('\\nüìã Latest Results (tail):')\n",
    "print(df_log.tail(5).to_string(index=False))\n",
    "\n",
    "print('\\nüìÇ Files in OUTDIR:')\n",
    "for p in sorted(glob.glob(os.path.join(OUTDIR, '*'))):\n",
    "    try:\n",
    "        sz = os.path.getsize(p)\n",
    "    except Exception:\n",
    "        sz = -1\n",
    "    print(f'{p}  ({sz} bytes)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
